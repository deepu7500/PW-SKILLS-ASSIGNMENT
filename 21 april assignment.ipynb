{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88f97154",
   "metadata": {},
   "source": [
    "# ans 1-\n",
    "Euclidean distance metrics\n",
    "The Euclidean distance is a straight-line distance between two points in a multidimensional space. For two points (x1, y1) and (x2, y2) in a 2-dimensional space, the Euclidean distance is calculated as:\n",
    "    Distance = âˆš((x2 - x1)^2 + (y2 - y1)^2)\n",
    "\n",
    "Manhattan Distance metrics\n",
    "The Manhattan distance is the sum of the absolute differences between the coordinates of two points in a multidimensional space. For two points (x1, y1) and (x2, y2) in a 2-dimensional space, the Manhattan distance is calculated as:\n",
    "    Distance = |x2 - x1| + |y2 - y1|\n",
    "    \n",
    "   \n",
    "The Euclidean distance is sensitive to the scaling of features since it considers both the magnitude and direction of differences between points.the Manhattan distance is less sensitive to data scaling since it only considers the absolute differences between coordinates.\n",
    "The Euclidean distance represents the actual geometric distance between points, capturing the underlying shape of the feature space.This is useful when the relationship between features is more intuitive and should be captured in the distance calculation.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23615c29",
   "metadata": {},
   "source": [
    "# ans 2-\n",
    "\n",
    "Selecting the optimal value of k for a KNN classifier or regressor is an important task in order to achieve the best performance. The choice of k significantly affects the model's accuracy, bias-variance tradeoff, and generalization capabilities.\n",
    "Some technique which determine k values are-\n",
    "* cross validation- The k value that results in the best average cross-validation performance is considered the optimal k.\n",
    "* Grid Search- Grid search involves creating a grid of k values to explore and evaluating the model's performance for each k using a validation set. \n",
    "* Domain Knowledge- In some cases, domain knowledge or prior experience might suggest a reasonable range for k.\n",
    "* Statistical Measures: Some statistical measures, such as the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC), can be used to select the optimal k value for KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e47e4af",
   "metadata": {},
   "source": [
    "# ans 3-\n",
    "The choice of distance metric in a K-Nearest Neighbors (KNN) classifier or regressor significantly affects its performance, as it determines how instances are measured and compared to make predictions. The distance metric defines the \"closeness\" or similarity between data points in the feature space, which is fundamental to how KNN operates.\n",
    "the choice of distance metric should be guided by your understanding of the data and the problem domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db827d7",
   "metadata": {},
   "source": [
    "# ans 4-\n",
    "K-Nearest Neighbors (KNN) classifiers and regressors have several important hyperparameters that influence their performance.\n",
    "\n",
    "* K - This hyperparameter determines how many neighboring data points are considered when making predictions. A smaller K can lead to more flexible models that are sensitive to noise, while a larger K can result in smoother predictions but might miss local patterns.\n",
    "* Distance Metric- The choice of distance metric (e.g., Euclidean, Manhattan, cosine) affects how data points are compared. Different metrics are suitable for different types of data and domains.\n",
    "* Algorithm- There are two main algorithms for finding neighbors in KNN: brute force and KD-Tree. The brute-force method computes distances to all data points. KD-Tree is a more efficient way to search for neighbors.\n",
    "\n",
    "\n",
    "To tune these hyperparameters and improve model performance:\n",
    "\n",
    "* Feature Scaling: KNN is sensitive to feature scales, so consider scaling your features to ensure that no dimension dominates the distance calculation.\n",
    "* Ensemble Methods: You can also use ensemble methods like Bagging or Boosting with KNN to improve its performance by reducing variance or bias.\n",
    "* Outlier Handling: Since KNN can be sensitive to outliers, you might want to preprocess or handle outliers before training your model.\n",
    "* Cross-Validation: Use techniques like k-fold cross-validation to evaluate your model's performance on different subsets of the data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a697507",
   "metadata": {},
   "source": [
    "# ans 5-\n",
    "The size of the training set can have a significant impact on the performance of a KNN classifier or regressor.\n",
    "\n",
    "Effects of Training Set Size:\n",
    "\n",
    "* Small Training Set: With a small training set, the model might overfit because it could rely too heavily on noise or outliers. \n",
    "\n",
    "* A larger training set can help the model better capture the underlying distribution of the data. It can reduce the influence of noise and outliers, leading to more robust predictions. \n",
    "\n",
    "techniques can be used to optimize the size of the training set-\n",
    "\n",
    "* Outlier Removal: If your dataset contains outliers, removing them can improve the quality of your training data.\n",
    "\n",
    "* Feature Selection: If your dataset has many features, feature selection techniques can help reduce the dimensionality of the problem, making it easier to work with a smaller training set.\n",
    "\n",
    "* Subsampling: If your dataset is very large, you can consider using a subset of the data for training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90bc9cc",
   "metadata": {},
   "source": [
    "# ans 6-\n",
    "* KNN is senstive to noisy data and outliers as it affect distance calculation and prediction so processing is used to remove or handle outliers.\n",
    "* as no. of dimesions increase data become less informative PCA is used to reduce no. of features.\n",
    "* choosing value is also impact model as it not correct always so elbow methode is used to find best k value.\n",
    "* KNN also face imbalace data set problem resampling texhniques used to balance dataset.\n",
    "8 KNN not define important feature we use feature selection .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e04504e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
