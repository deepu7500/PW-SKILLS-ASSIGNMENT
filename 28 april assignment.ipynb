{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1427327c",
   "metadata": {},
   "source": [
    "# ans  1\n",
    "Hierarchical clustering is a clustering technique used in data analysis and machine learning to group similar data points into clusters based on their similarity or distance. It creates a hierarchy of clusters by iteratively merging or splitting clusters, ultimately forming a tree-like structure known as a dendrogr\n",
    "This dendrogram represents the relationships between the data points and clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad194c36",
   "metadata": {},
   "source": [
    "# ans 2-\n",
    " Two main types of hierarchical clustering algorithms are-\n",
    "* Agglomerative Clustering \n",
    "* Divisive Clustering\n",
    "\n",
    "Agglomerative Clustering- \n",
    "Agglomerative clustering starts with individual data points as their own clusters and gradually merges them together. \n",
    "\n",
    "Divisive Clustering-\n",
    "Divisive clustering, also known as \"top-down clustering,\" takes the opposite approach. It starts with all data points in a single cluster and then recursively divides them into smaller clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61989d9",
   "metadata": {},
   "source": [
    "# ans 3-\n",
    "The distance between two clusters in hierarchical clustering is determined by using a distance metric that quantifies the dissimilarity or similarity between the data points within the clusters.\n",
    "some common distance metrics used in hierarchical clustering-\n",
    "\n",
    "* Euclidean Distance\n",
    "This is the most common distance metric. It calculates the straight-line distance between two data points in a Euclidean space. \n",
    "  \n",
    "  Distance = âˆš((x2 - x1)^2 + (y2 - y1)^2)\n",
    "\n",
    "* Cosine Similarity: Used to measure the cosine of the angle between two vectors, it's often used in text or document clustering.\n",
    "    \n",
    "    cosine similarity(p,q)= p.q/|p|.|q|\n",
    "\n",
    "* Manhattan Distance metrics The Manhattan distance is the sum of the absolute differences between the coordinates of two points in a multidimensional space. For two points (x1, y1) and (x2, y2) in a 2-dimensional space, the Manhattan distance is calculated as: \n",
    "    \n",
    "    Distance = |x2 - x1| + |y2 - y1|\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ec0060",
   "metadata": {},
   "source": [
    "# ans 4-\n",
    " The optimal number of clusters is somehow subjective and depends on the method used for measuring similarities and the parameters used for partitioning.\n",
    " \n",
    "Elbow Method- The idea is to find the point where adding more clusters starts to provide diminishing returns in terms of reducing the sum of squares. The \"elbow point\" on the plot is often considered a reasonable choice for the number of clusters.\n",
    "\n",
    "Silhouette Score- It ranges from -1 to 1, where higher values indicate better-defined clusters. Plotting the silhouette score against the number of clusters can help identify an optimal number that maximizes the score.\n",
    "\n",
    "Dendrogram- the dendrogram provides a visual representation of how clusters are merged or divided. By analyzing the dendrogram, you can look for significant jumps in the distance values between successive merges. A larger jump might indicate a suitable number of clusters.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c73222c",
   "metadata": {},
   "source": [
    "# ans 5-\n",
    "A dendrogram is a diagram that shows the hierarchical relationship between objects. It is most commonly created as an output from hierarchical clustering. The main use of a dendrogram is to work out the best way to allocate objects to clusters.\n",
    "\n",
    "They help you understand the relationships and structure in your data, decide on the appropriate number of clusters, and identify potential outliers or anomalies. However, interpreting dendrograms can require some experience and domain knowledge, especially when making decisions about cutting the dendrogram or extracting.\n",
    "\n",
    "they are useful as analysing -\n",
    "* vertical axis\n",
    "* horizontal axis\n",
    "* identifying clusters\n",
    "* camparison and validation\n",
    "* intrpreting relationship\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee79ae4",
   "metadata": {},
   "source": [
    "# ans 6\n",
    "Yes, hierarchical clustering can be used for both numerical and categorical data.\n",
    "\n",
    "For Numerical Data-\n",
    "Distance metrics used for numerical data. these are of three types as we discussed earlier.\n",
    "\n",
    "For Categorical Data- \n",
    "\n",
    "* Categorical Distance Metrics: Customized metrics can be developed based on the specific nature of the categorical variables, for example, using a chi-squared test statistic.\n",
    "*Jaccard Distance: Measures the dissimilarity between two sets by calculating the size of the intersection divided by the size of the union.\n",
    "* Hamming Distance: Used for strings of equal length, it measures the number of positions at which the corresponding elements differ.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14c67ab",
   "metadata": {},
   "source": [
    "# ans 7-\n",
    "Hierarchical clustering can be used to identify outliers or anomalies in your data by analyzing the structure of the dendrogram and the distances between clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6515f496",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
